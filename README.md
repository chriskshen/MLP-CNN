In this study, we investigate a critical question in image classification: How effectively do different neural network architectures—Multi-Layer Perceptrons (MLPs) and Convolutional Neural Networks (CNNs)—handle classification tasks of varying complexity, and is increased architectural complexity always necessary for improved performance? To address this, we empirically evaluate three simple architectures on two benchmark datasets, MNIST and EMNIST-balanced, which present progressively challenging classification tasks.

Initially, we tested the best-performing simple MLP (three dense layers) and CNN (two convolutional layers followed by one dense layer) on both datasets. Surprisingly, both models achieved nearly equivalent results: accuracy >98.2% for MNIST within 1 minute 4 seconds and 2 minutes 44 seconds, and >85.6% for EMNIST within 36 seconds and 51 seconds, respectively. To address this unexpected outcome, Dr. AJ recommended starting with simpler models to better understand the role of different architectures.

The simplified CNN architecture consists of a single convolutional layer with 4 filters, followed by a 32-neuron dense layer and a dropout layer. The two-layer MLP includes two 32-neuron dense layers, each followed by a dropout layer, while the single-layer MLP (MLP0) uses one 32-neuron dense layer and a dropout layer.

Our results, as shown in the figure below, reveal that while MLPs can achieve reasonable accuracy on simple datasets like MNIST, they struggle to handle the higher complexity of EMNIST due to their limited ability to capture spatial hierarchies. CNNs, with their local feature extraction and spatial modeling capabilities, consistently outperform MLPs in terms of both training and validation accuracy on both datasets. For EMNIST, the CNN achieves a validation accuracy of 80.39% within 80 epochs, compared to 71.47% for the two-layer MLP and 74.06% for the single-layer MLP0. On MNIST, the CNN reaches a validation accuracy of 93.35% in 80 epochs, whereas the two-layer MLP and single-layer MLP0 achieve 73.22% and 47.87%, respectively, as indicated by the red dashed lines.

Interestingly, extending training epochs improves MLP performance, with the two-layer MLP achieving 88.91% validation accuracy after 300 epochs on MNIST. While the training durations for 80 epochs of the CNN, 300 epochs of the two-layer MLP, and 300 epochs of the single-layer MLP0 are comparable, the learning curves reveal distinct delays before achieving noticeable learning gains. Specifically, the CNN shows a learning delay of approximately 10 epochs, the two-layer MLP about 25 epochs, and the single-layer MLP0 around 50 epochs. This underscores CNNs' ability to efficiently learn robust spatial features, as they quickly achieve significant accuracy improvements relative to MLPs.

In contrast, no such learning delays were observed for the EMNIST dataset, where the CNN and MLPs both exhibit relatively consistent early learning dynamics. Overall, the CNN consistently outperforms MLPs on both datasets, demonstrating superior training and validation accuracy due to its local feature extraction and spatial modeling capabilities. These findings highlight the trade-offs between architectural design, training duration, and computational efficiency, with CNNs offering a compelling balance for datasets requiring spatial feature learning.

![Picture2=Simple Models](https://github.com/user-attachments/assets/370b2662-de8e-4d30-b490-2bb3ef65e69d)
